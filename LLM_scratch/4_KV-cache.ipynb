{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7aa3a32-9cd4-4b17-8969-2c4e1a768041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42aa5e9-31cf-4cc2-92c7-b187c41dc773",
   "metadata": {},
   "source": [
    "### Multi-Head Attention with simple KV Cache\n",
    "- Approach 2)\n",
    "    1. simple) Use torch.cat\n",
    "    2. typical) Preallocate [Batch, num_head, max_len, head_dim] fixed buffer allocates, then inplace\n",
    "- Optimization: PagedAttention, Quantization (KV: FP16, Q: FP32), FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b45f6f4-facb-4555-923b-1197a67c70de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_rate, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert d_out%num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out//num_heads # 768//12 == 64\n",
    "\n",
    "        self.Q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.K = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.V = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.proj_out = nn.Linear(d_out, d_out) # to combine head outputs\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1),\n",
    "            persistent=False\n",
    "        ) # non trainable parameter, but sort of model's state\n",
    "\n",
    "        ####################################################\n",
    "        # KV Cache: define in the buffer\n",
    "        self.register_buffer(\"cache_k\", None, persistent=False)\n",
    "        self.register_buffer(\"cache_v\", None, persistent=False)\n",
    "        ####################################################\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        batch, num_tokens, d_in = x.shape\n",
    "\n",
    "        # 1) get q, k ,v\n",
    "        # ==> d_out = num_heads * head_dim \n",
    "        q, k, v = self.Q(x), self.K(x), self.V(x) # [batch, num_tokens, d_out]\n",
    "\n",
    "        # 2) split qkv into the head\n",
    "        # ==> [batch, num_tokens, num_heads, head_dim]\n",
    "        q = q.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        k_new = k.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        v_new = v.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        ####################################################\n",
    "        # KV Cache: based on num_tokens (batch, num_heads, head_dim are fixed)\n",
    "        ## but) torch.cat operation is inefficent due to inability of resize --> O(L^2)\n",
    "        if use_cache:\n",
    "            if not self.cache_k:\n",
    "                self.cache_k, self.cache_v = k_new, v_new\n",
    "            else:\n",
    "                self.cache_k = torch.cat([self.cache_k, k_new], dim=1) # along num_heads\n",
    "                self.cache_v = torch.cat([self.cache_v, v_new], dim=1)\n",
    "            k, v = self.cache_k, self.cache_v\n",
    "        else:\n",
    "            k, v = k_new, v_new\n",
    "        ####################################################\n",
    "        \n",
    "        # 3) [batch, num_heads, num_tokens, head_dim]\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        # 4) get attention score [batch, num_heads, num_tokens, num_tokens]\n",
    "        attn_score = q @ k.transpose(2,3)\n",
    "\n",
    "        # 5) masking \n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_score.maksed_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # 6) get attention weight (normalization)\n",
    "        attn_weight = torch.softmax(attn_score/k.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        # 7) dropout \n",
    "        attn_weight = self.dropout(attn_weight)\n",
    "\n",
    "        # 8) get context vector \n",
    "        # ==> [batch, num_heads, num_tokens, head_dim] -> [batch, num_tokens, num_heads, head_dim]\n",
    "        context_vec = (attn_weight @ v).transpose(1,2)\n",
    "\n",
    "        # 9) combine head to make original shape\n",
    "        context_vec = context_vec.reshape(batch, num_tokens, self.d_out)\n",
    "        context_vec = self.proj_out(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "    ####################################################\n",
    "    # KV Cache: reset\n",
    "    def reset_cache(self):\n",
    "        self.cache_k, self.cache_v = None, None\n",
    "    ####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e56f8-377e-4e02-8ac7-65d5cdd15a86",
   "metadata": {},
   "source": [
    "### Transformer Architecture\n",
    "1. Layer Normalization\n",
    "2. GELU + Feed forward\n",
    "3. Skip Connection in Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19278c3-2d5d-4eb9-8dfc-a906663d8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x-meam)/torch.sqrt(var + self.eps)\n",
    "        return self.scale*norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9197afd2-c784-4fdc-9579-b0365d2900df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd11ff3c-2c1e-44d0-a6df-6497512993b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emd_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4*emb_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(4*emb_dim, emb_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e2ddac2-9b35-4532-9d51-dc751b1916f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            drop_rate = cfg[\"drop_rate\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"])\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.ff = FeedForward(cfg[\"emb_dim\"])\n",
    "        self.dropAdd = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "    def forward(self, x, use_cache=False):\n",
    "        skip_ = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x, use_cache=use_cache)\n",
    "        x = dropAdd(x)\n",
    "        x += skip_\n",
    "\n",
    "        skip_ = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = dropAdd(x)\n",
    "        x += skip_\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5821e-f37a-4bfc-bd1b-0b967420ad83",
   "metadata": {},
   "source": [
    "### GPT\n",
    "1. token embedding\n",
    "2. positional embedding\n",
    "3. dropout\n",
    "4. transformer block x 12\n",
    "5. final layernorm\n",
    "6. linear output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62c1844-4c5e-4406-bd0a-dfe98c0fb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        ####################################################\n",
    "        # KV cache\n",
    "        self.trf_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.current_pos = 0\n",
    "        ####################################################\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, use_cache=False):\n",
    "        batch, seq_len = in_idx.shape\n",
    "\n",
    "        # 1st) Token embedding\n",
    "        tok_embeds = self.tok_emb(in_idx) \n",
    "        \n",
    "        # 2nd) positional encoding\n",
    "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        ####################################################\n",
    "        # KV cache:\n",
    "        if use_cache:\n",
    "            pos_ids = torch.arange(self.current_pos, self.current_pos + seq_len, device=in_idx.device, dtype=torch.long)\n",
    "            self.current_pos += seq_len\n",
    "        else:\n",
    "            pos_ids = torch.arange(0, seq_len, device=in_idx.device, dtype=torch.long)\n",
    "        pos_embeds = self.pos_emb(pos_ids).unsqueeze(0)\n",
    "        ####################################################\n",
    "        x = tok_embeds + pos_embeds\n",
    "\n",
    "        # 3rd) Dropout\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # 4th) trf block\n",
    "        # x = self.trf_block(x)\n",
    "        ####################################################\n",
    "        # KV cache: change use_cache in the transformer block as well\n",
    "        for blk in self.trf_blocks:\n",
    "            x = blk(x, use_cache=use_cache)\n",
    "        ####################################################\n",
    "\n",
    "        # 5th) final layer norm\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # 6th) linear projection\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def reset_kv_cache(self):\n",
    "        for blk in self.trf_blocks:\n",
    "            blk.att.reset_cache()\n",
    "        self.current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6010f8-5ee8-4a41-bc4e-7ffe88b8da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple_cached(model, idx, max_new_tokens, use_cache=True):\n",
    "    model.eval()\n",
    "\n",
    "    ctx_len = model.pos_emb.num_embeddings\n",
    "    if use_cache:\n",
    "        model.reset_kv_cache() # initial cache\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx[:, :-ctx_len:], use_cache=True)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "            idx = torch.cat([idx, next_idx], dim=1)\n",
    "            with torch.no_grad():\n",
    "                logits = model(next_idx, use_cache=True)\n",
    "    else:\n",
    "        for _ in range(max_new_tokens):\n",
    "            with torch.no_grad():\n",
    "                logits = model(idx[:, -cnt_len:], use_cache=False)\n",
    "            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "            idx = torch.cat([idx, next_idx], dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5548dea-bb52-4237-96f3-664c63f3bdba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
